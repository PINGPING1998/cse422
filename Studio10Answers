1} Tara Renduchintala and Anderson Gonzalez

2} In Kernelshark, when we filtered the trace.dat file by "workload" we saw
that processes were running on all four of the cores. Therefore, this task
was being used on all of the cores for certain.

3} 
#define UNLOCKED 0
#define LOCKED 1

int lock_state=UNLOCKED;

4}
//Lock function
void lock(volatile int * x){
	int expected = UNLOCKED;
	int desired = LOCKED;
	bool status = false;
	status = __atomic_compare_exchange( &lock_state, &expected, &desired, 0, __ATOMIC_ACQ_REL, __ATOMIC_ACQUIRE);	
	if(!status){
		printf("Error occured in Lock\n");
	}
	while(!status){
		expected = UNLOCKED;
		status = __atomic_compare_exchange( &lock_state, &expected, &desired, 0, __ATOMIC_ACQ_REL, __ATOMIC_ACQUIRE);	
	}
}

//Unlock function
void unlock(volatile int * y){
	int expected = LOCKED;
	int desired = UNLOCKED;
	bool status = true;
	status = __atomic_compare_exchange( &lock_state, &expected, &desired, 0, __ATOMIC_ACQ_REL, __ATOMIC_ACQUIRE);
	if(!status){
		printf("Error occured in Unlock");
	}
}

5} The "CPU Finished!" print statement in the original version all appeared
at the same time as they were all sharing the critical section. When the locking
mechanism was in place, each print statement printed out one after the other, indicating
only one CPU was running the critical_section() function at a time.


